{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction\n",
    "\n",
    "This notebook assumes that you have read through Jake Vanderplas' notebook on [dimensionality reduction](https://github.com/jakevdp/sklearn_pycon2015/blob/master/notebooks/04.1-Dimensionality-PCA.ipynb).  Here, we will expand on some of the ideas presented in the notebook.  First, we'll drill down into the underlying mathematics of principal components analysis (PCA), we'll apply PCA to some non-digit data (gasp), and then we'll move to non-linear dimensionality reduction.\n",
    "\n",
    "Suppose we have a dataset of $x_1, x_2, \\ldots, x_n$ with each $x_i \\in \\mathbb{R}^d$.  We'd like to find the most \"important dimensions\" in the data.  Here are some criteria that we can use to operationalize this notion of importance:\n",
    "1.  Compress the data to a fixed size while preserving as much information as possible.\n",
    "2.  Maximize the variance of the data when projected into this reduced space.\n",
    "\n",
    "For the case of linear dimensionality reduction, it turns out these two notions are equivalent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Dimensionality Reduction\n",
    "\n",
    "First, let's make the problem a little simpler.  Let's consider the case where the mean of the dataset for each of the d-dimensions is zero.  We can accomplish this by simply computing the mean across all of the points $x_1, \\ldots, x_n$ and then subtracting the mean from each point to create a new, zero-mean dataset.\n",
    "\n",
    "Let's consider the task of reducing our dataset to a single number for each data point.  Specifically, we will consider schemes that project all datapoints onto a vector $v$.\n",
    "\n",
    "One way to think of our goal is that we'd like to choose a $v$ that preserves as much of the original information in the data as possible.  A sensible way to define this preservation of information is to look at the squared error between the projection onto $v$ and the original data point.  The projection of a point onto a vector $v$ can be understood geometrically using this picture (source [Wikipedia entry on Vector Projection](https://en.wikipedia.org/wiki/Vector_projection)).\n",
    "![vector projection](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3e/Dot_Product.svg/300px-Dot_Product.svg.png)\n",
    "\n",
    "This is starting to shape up, but we don't know what $\\theta$ is.  In order to do this we will have to apply the following fact about dot products.\n",
    "\n",
    "$$a \\cdot b = |a| |b| \\cos \\theta $$\n",
    "\n",
    "Therefore, $cos \\theta = \\frac{a \\cdot b}{|a||b|}$.  From our picture above, we know that the vector we'd like to compute is $\\hat{B} |A| \\cos \\theta$ (where $\\hat{B}$ is a unit vector in the direction of $B$).  Another way to write the dot-product of $A$ and $B$ is as a vector-vector multiplication: $A^\\top B$.  Substituing in our formula for $\\cos \\theta$ yields:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\hat{B} |A| \\cos \\theta &= \\hat{B} |A| \\frac{A^\\top B}{|A||B|} \\\\\n",
    "&= \\hat{B} \\frac{A^\\top B}{|B|} \\\\\n",
    "&= \\hat{B} \\frac{B^\\top A}{|B|}, \\mbox{note: dot product is commutative}\n",
    "\\end{align}$$\n",
    "\n",
    "Therefore, we can define the squared error for a particular choice of $v$ as:\n",
    "\n",
    "$$\\left |\\hat{v} \\frac{v^\\top x_i}{|v|}- x_i \\right|^2 \\enspace .$$\n",
    "\n",
    "One thing that is clear is that the magnitude of $v$ doesn't really have any effect on the reconstruction error.  Therefore, we can restrict ourselves to looking for vectors $v$ that are unit vectors.  This simplifies our formula for the reconstruction error to:\n",
    "\n",
    "$$\\left | v v^\\top x_i - x_i \\right|^2 \\enspace .$$\n",
    "\n",
    "The squared magnitude of a vector, $|u|^2$, can be computed as $u^\\top u$.  We can now write the reconstruction error as:\n",
    "\n",
    "$$\\left ( v v^\\top  x_i - x_i \\right )^\\top \\left ( v v^\\top  x_i - x_i \\right )$$\n",
    "$$ =x_i^\\top v v^\\top v v^\\top x_i - 2x_i^\\top v v^\\top x_i + x_i^\\top x_i$$\n",
    "\n",
    "Since $v$ is a unit vector, $v^\\top v = 1$.  Our reconstrution can be further simplied to:\n",
    "\n",
    "$$\\begin{align}\n",
    "&= x_i^\\top v v^\\top x_i - 2x_i^\\top v v^\\top x_i + x_i^\\top x_i \\\\\n",
    "&= - x_i^\\top v v^\\top x_i + x_i^\\top x_i\n",
    "\\end{align} \\enspace .$$\n",
    "\n",
    "Making use of the commutative property for dot products ($a^\\top b = b^\\top a$).  We have:\n",
    "\n",
    "$$ = - v^\\top x_i x_i^\\top v + x_i^\\top x_i$$\n",
    "\n",
    "The cumulative squared error over the whole dataset is given as:\n",
    "\n",
    "$$\\begin{align}\n",
    "error(v) &= \\sum_{i=1}^n \\left ( x_i^\\top x_i - v^\\top x_i x_i^\\top v \\right ) \\\\\n",
    "&= \\sum_{i=1}^n  x_i^\\top x_i  - \\sum_{i=1}^n v^\\top x_i x_i^\\top v \\\\\n",
    "&= \\sum_{i=1}^n  x_i^\\top x_i  - v^\\top \\left (\\sum_{i=1}^n  x_i x_i^\\top \\right ) v\n",
    "\\end{align}$$\n",
    "\n",
    "We are interested in the vector $v$ that minimizes our error function:\n",
    "\n",
    "$$\\begin{align}\n",
    "v^\\star &= \\arg\\min_v error(v) \\\\\n",
    "&= \\arg\\min_v \\sum_{i=1}^n  x_i^\\top x_i  - v^\\top \\left (\\sum_{i=1}^n  x_i x_i^\\top \\right ) v \\\\\n",
    " &= \\arg\\min_v- v^\\top \\left (\\sum_{i=1}^n  x_i x_i^\\top \\right ) v \\\\\n",
    "&= \\arg\\max_v v^\\top \\left (\\sum_{i=1}^n  x_i x_i^\\top \\right ) v \n",
    "\\end{align}$$\n",
    "\n",
    "The last equation is simply the definition of the maximum Eigenvalue of the matrix $\\left (\\sum_{i=1}^n  x_i x_i^\\top \\right )$ (also known as the [Rayleigh coefficient](https://en.wikipedia.org/wiki/Rayleigh_quotient)).  The maximum Eigenvalue of this matrix can be computed easily using a numerical solver.  This gives us the first principal component.  If you want more principal components, you simply subtract out the reconstruction of the original data using the first principal component and repeat the procedure defined above on this transformed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigenfaces\n",
    "\n",
    "One of the most widely used examples of PCA is as applied to faces.  This is the so-called \"Eigenfaces\" algorithm.  The approach is so widely used since it has a computational benefit, faces can be represented with just a few numbers, and it has the ability to isolate potentially uninteresting factors such as lighting conditions.\n",
    "\n",
    "I will show an example of this applied to the smile dataset that we saw way back in the bottom-up machine learning portion of the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../day06')\n",
    "from load_smiles import load_smiles\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# load the data\n",
    "data = load_smiles()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# make the data mean-zero\n",
    "scaler = StandardScaler(with_std=False)\n",
    "X_mean_zero = scaler.fit_transform(X)\n",
    "\n",
    "# verify that the standard scalar did its job\n",
    "print \"data shape\", X_mean_zero.shape\n",
    "print \"pixel means\", X_mean_zero.mean(axis=0)\n",
    "\n",
    "# compute first 36 Eigenfaces from our dataset\n",
    "pca = PCA(n_components=36)\n",
    "transformed = pca.fit_transform(X_mean_zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_images(images):\n",
    "    \"\"\" Adapted from Jake Vanderplas' scikit learn tutorials. \"\"\"\n",
    "    fig, axes = plt.subplots(6, 6, figsize=(24, 24))\n",
    "    fig.subplots_adjust(hspace=0.1, wspace=0.1)\n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        ax.imshow(images[i,:].reshape((24,24)).T, cmap='gray')\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "\n",
    "show_images(pca.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These principal components, \"Eigenface\", represent the best possible linear compression of the face database into just 36 numbers.  At a high-level they seem to capture things like lighting, pose, and facial expression.  It's possible that these faces would make for a more robust representation of a face for things like facial expression recognition and face identity recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-negative Matrix Factorization\n",
    "\n",
    "Another way to reduce the dimensionality of data is to apply non-negative matrix factorization.  This is very similar to PCA except all of the coefficients and all of the basis vectors must be positive.  This forces the effect of each component to be additive.  This has the effect of the algorithm focusing on extracting parts of objects rather than global effects as we see in PCA.\n",
    "\n",
    "For an overview of the algorithm and some cool figures see [this](http://www.columbia.edu/~jwp2128/Teaching/W4721/papers/nmf_nature.pdf) paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# warning... This code seems to be too slow to run in a reasonable\n",
    "# length of time.  We'll just jump right to the punchline by looking\n",
    "# at the paper.\n",
    "\n",
    "from sklearn.decomposition.nmf import NMF\n",
    "\n",
    "decomp = NMF()\n",
    "transformed = decomp.fit_transform(X)\n",
    "show_images(decomp.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# This import is needed to modify the way figure behaves\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "Axes3D\n",
    "\n",
    "from sklearn import manifold, datasets\n",
    "\n",
    "X, color = datasets.samples_generator.make_swiss_roll(n_samples=1500)\n",
    "X_r, err = manifold.locally_linear_embedding(X, n_neighbors=12,\n",
    "                                             n_components=2)\n",
    "X_pca = PCA(n_components=2).fit_transform(X)\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# Plot result\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(131, projection='3d')\n",
    "ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=color, cmap=plt.cm.Spectral)\n",
    "ax.set_title(\"Original data\")\n",
    "\n",
    "ax = fig.add_subplot(132)\n",
    "ax.scatter(X_r[:, 0], X_r[:, 1], c=color, cmap=plt.cm.Spectral)\n",
    "plt.xticks([]), plt.yticks([])\n",
    "ax.set_title('LLE')\n",
    "\n",
    "ax = fig.add_subplot(133)\n",
    "ax.scatter(X_pca[:, 0], X_pca[:, 1], c=color, cmap=plt.cm.Spectral)\n",
    "plt.xticks([]), plt.yticks([])\n",
    "ax.set_title('PCA')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimensionality reduction is a very large field.  Check out [this](https://lvdmaaten.github.io/publications/papers/JMLR_2008.pdf) great paper that describes one of the most promising new approaches (conveniently built into sklearn), and also surveys a bunch of older ones."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
